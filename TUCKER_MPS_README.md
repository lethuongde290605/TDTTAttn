# Tucker-MPS Compression for OPT Models

ÄÃ¢y lÃ  implementation cá»§a thuáº­t toÃ¡n nÃ©n Tucker-MPS cho cÃ¡c mÃ´ hÃ¬nh OPT, Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ thay tháº¿ cho EigenAttention compression.

## ğŸ“ Cáº¥u trÃºc Files

```
decompose/
  â”œâ”€â”€ tucker_mps_utils.py     # CÃ¡c hÃ m tiá»‡n Ã­ch cho Tucker vÃ  MPS decomposition
  â””â”€â”€ tucker_mps.py           # Main compression function cho OPT models

models/
  â””â”€â”€ decompose_modules.py    # Chá»©a OPTTuckerMPSAttention vÃ  OPTTuckerMPSDecoderLayer

test_tucker_mps.py            # Script Ä‘á»ƒ test compression
```

## ğŸ”§ CÃ¡ch hoáº¡t Ä‘á»™ng

### 1. Tucker Decomposition (HOOI Algorithm)
Tucker decomposition phÃ¢n tÃ¡ch tensor thÃ nh:
- **Core tensor**: Chá»©a thÃ´ng tin cáº¥u trÃºc chÃ­nh
- **Factor matrices**: Ma tráº­n biáº¿n Ä‘á»•i cho má»—i mode

```python
X â‰ˆ G Ã—â‚ Uâ‚ Ã—â‚‚ Uâ‚‚ Ã—â‚ƒ Uâ‚ƒ
```

### 2. MPS (Matrix Product States) Decomposition
MPS decomposition cho tensor 3D:
```
X â†’ [Bâ‚, G, Bâ‚‚]
```
Vá»›i cÃ¡c tensor cores Ä‘Æ°á»£c káº¿t ná»‘i tuáº§n tá»±.

### 3. Combined Tucker-MPS
Thuáº­t toÃ¡n káº¿t há»£p:
1. Reshape weight matrix 768Ã—768 â†’ 64Ã—64Ã—144
2. Ãp dá»¥ng MPS decomposition vá»›i threshold `mps_eps`
3. Ãp dá»¥ng HOOI (Tucker) vá»›i ranks cho má»—i mode
4. Tensor contraction Ä‘á»ƒ tÃ¡i táº¡o compressed weight
5. Reshape vá» 768Ã—d (d < 768)

## ğŸš€ Sá»­ dá»¥ng

### Test Ä‘Æ¡n giáº£n compression function:

```bash
python test_tucker_mps.py --test-type compression-only
```

### Test full layer compression:

```bash
python test_tucker_mps.py --test-type full
```

### TÃ­ch há»£p vÃ o training pipeline:

```python
from decompose.tucker_mps import tucker_mps_compress

# Trong main script cá»§a báº¡n:
compressed_model = tucker_mps_compress(
    lm=language_model,
    args=args,
    dataloader=calib_dataloader,
    logger=logger,
    mps_eps=0.99,        # Threshold cho MPS (0.9-0.99)
    hooi_ranks=[6, 6, 8] # Ranks cho Tucker decomposition
)
```

## ğŸ›ï¸ Hyperparameters

### `mps_eps` (MPS Epsilon Threshold)
- **Range**: 0.9 - 0.99
- **Higher values** (0.99): Giá»¯ nhiá»u thÃ´ng tin hÆ¡n, nÃ©n Ã­t hÆ¡n
- **Lower values** (0.90): NÃ©n máº¡nh hÆ¡n, cÃ³ thá»ƒ máº¥t thÃ´ng tin
- **Recommended**: 0.99 cho láº§n test Ä‘áº§u

### `hooi_ranks` (Tucker Decomposition Ranks)
- **Format**: `[r1, r2, r3]` - ranks cho 3 modes
- **Default**: `[6, 6, 8]`
- **Higher ranks**: Giá»¯ nhiá»u thÃ´ng tin hÆ¡n, nÃ©n Ã­t hÆ¡n
- **Lower ranks**: NÃ©n máº¡nh hÆ¡n
- **Examples**:
  - `[6, 6, 8]` - Moderate compression
  - `[5, 5, 6]` - Higher compression
  - `[4, 4, 5]` - Aggressive compression

## ğŸ“Š Expected Results

Vá»›i OPT-125m (hidden_size=768):

| Configuration | Compression Ratio | Space Saved | Reconstruction Error |
|--------------|------------------|-------------|---------------------|
| eps=0.99, ranks=[6,6,8] | ~0.40-0.50 | 50-60% | < 0.1 |
| eps=0.95, ranks=[5,5,6] | ~0.30-0.40 | 60-70% | < 0.15 |
| eps=0.90, ranks=[4,4,5] | ~0.25-0.35 | 65-75% | < 0.20 |

## ğŸ” Classes Overview

### `OPTTuckerMPSAttention`
Thay tháº¿ cho `OPTAttention` vá»›i compressed weights:
- NÃ©n K, Q, V projection matrices
- Xá»­ lÃ½ compressed dimensions trong forward pass
- Tá»± Ä‘á»™ng táº¡o projection layers khi cáº§n

### `OPTTuckerMPSDecoderLayer`
Thay tháº¿ cho `OPTDecoderLayer`:
- Sá»­ dá»¥ng `OPTTuckerMPSAttention`
- Giá»¯ nguyÃªn FFN vÃ  LayerNorm
- Compatible vá»›i OPT model architecture

## ğŸ§ª Testing Workflow

### Step 1: Test compression function
```bash
python test_tucker_mps.py --test-type compression-only
```
Kiá»ƒm tra:
- âœ“ Compression function cháº¡y khÃ´ng lá»—i
- âœ“ Output shape Ä‘Ãºng
- âœ“ Reconstruction error reasonable

### Step 2: Test single layer
```bash
python test_tucker_mps.py --test-type full
```
Kiá»ƒm tra:
- âœ“ Layer initialization thÃ nh cÃ´ng
- âœ“ Forward pass khÃ´ng lá»—i
- âœ“ Output shape khá»›p vá»›i input
- âœ“ Compression ratio vÃ  error trong ngÆ°á»¡ng cháº¥p nháº­n

### Step 3: Test multiple configurations
Script sáº½ tá»± Ä‘á»™ng test vá»›i cÃ¡c configuration khÃ¡c nhau:
- eps = [0.99, 0.95, 0.90]
- ranks = [[6,6,8], [5,5,6], [4,4,5]]

### Step 4: Integrate vÃ o main pipeline
Sau khi test thÃ nh cÃ´ng, thÃªm vÃ o main training script.

## ğŸ› Troubleshooting

### Error: "Input tensor must have shape (768, 768)"
â†’ Check weight matrix shape. Tucker-MPS hiá»‡n táº¡i chá»‰ support 768Ã—768 (OPT-125m, OPT-350m, etc.)
â†’ CÃ³ thá»ƒ modify `combined_mps_hooi_compression()` Ä‘á»ƒ support sizes khÃ¡c

### Error: Shape mismatch trong forward pass
â†’ Check compressed dimensions compatibility
â†’ Verify projection layers Ä‘Æ°á»£c táº¡o Ä‘Ãºng

### High reconstruction error (> 0.2)
â†’ TÄƒng `mps_eps` (vÃ­ dá»¥ tá»« 0.90 â†’ 0.95)
â†’ TÄƒng `hooi_ranks` (vÃ­ dá»¥ tá»« [4,4,5] â†’ [6,6,8])

### Out of memory
â†’ Giáº£m batch size trong test
â†’ Test trÃªn GPU vá»›i memory lá»›n hÆ¡n
â†’ Compress tá»«ng layer má»™t thay vÃ¬ toÃ n bá»™ model

## ğŸ“ So sÃ¡nh vá»›i EigenAttention

| Aspect | EigenAttention | Tucker-MPS |
|--------|---------------|------------|
| Decomposition | SVD per head | Tucker + MPS combined |
| Adaptivity | Dynamic rank selection | Fixed ranks |
| Complexity | Per-head basis vectors | Global tensor decomposition |
| Memory | Lower during decompose | Higher during decompose |
| Compression | Head-wise | Global structure |

## ğŸ”œ Next Steps

1. **Test trÃªn calibration data thá»±c**
   - Sá»­ dá»¥ng dataset giá»‘ng EigenAttn (C4, WikiText, etc.)
   - Compare reconstruction error

2. **Evaluate downstream tasks**
   - Test perplexity trÃªn validation set
   - Run lm-eval benchmarks

3. **Optimize hyperparameters**
   - Grid search cho best eps vÃ  ranks
   - Balance giá»¯a compression vÃ  accuracy

4. **Extend sang models khÃ¡c**
   - Llama (hidden_size khÃ¡c 768)
   - MPT
   - Flexible size support

## ğŸ“š References

- Tucker Decomposition: Kolda & Bader, "Tensor Decompositions and Applications", 2009
- MPS: Matrix Product States from quantum physics
- HOOI: Higher-Order Orthogonal Iteration algorithm

## ğŸ¤ Contribution

Náº¿u muá»‘n extend hoáº·c improve:
1. Test vá»›i model sizes khÃ¡c (OPT-1.3B, OPT-2.7B)
2. Implement dynamic rank selection tÆ°Æ¡ng tá»± EigenAttn
3. Add support cho Llama, GPT-2, etc.
4. Optimize memory usage trong decomposition

---

**Author**: Based on EigenAttention architecture
**Date**: October 2025
